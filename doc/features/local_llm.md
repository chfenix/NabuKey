# 本地 LLM（离线对话）

> **状态**: 📋 规划中 | **优先级**: ⭐⭐ 低 | **复杂度**: 高 | **类型**: 可选高级功能

## 概述

在 Android 设备上运行小型 LLM 模型，实现离线对话能力。当 Home Assistant 不可用或网络断开时，可作为 Fallback 提供基础对话功能。

## 设备要求

| 项目 | 最低配置 | 推荐配置 |
|-----|---------|---------|
| RAM | 8GB | 12GB+ |
| 芯片 | 骁龙 800 系列 | 骁龙 8 Gen 1+ |
| 存储 | 4GB 可用 | 8GB+ 可用 |

**目标设备**: Redmi K40 (12GB RAM, 骁龙 870) ✅ 完全满足

## 技术选型

### 推荐模型

| 模型 | 参数量 | 内存需求 (Q4) | 速度 | 中文能力 | 推荐度 |
|-----|--------|--------------|------|---------|--------|
| **Qwen3 4B** | 4B | ~2.5GB | 8-12 tok/s | ⭐⭐⭐⭐⭐ 优秀 | ✅ **首选** |
| **Phi-3 Mini** | 3.8B | ~1.8GB | 10-15 tok/s | ⭐⭐⭐ 良好 | ✅ 推荐 |
| Qwen3 1.7B | 1.7B | ~1.2GB | 15-20 tok/s | ⭐⭐⭐⭐ 很好 | 轻量选择 |
| Gemma 2B | 2B | ~1.5GB | 12-18 tok/s | ⭐⭐⭐ 良好 | 备选 |

**首选方案**: **Qwen3 4B (Q4_K_M 量化)**
- 阿里开源，中文能力最强
- 4位量化后仅需 2.5GB 内存
- 可与 STT 同时运行

### 推理框架

| 框架 | 特点 | 推荐度 |
|-----|------|--------|
| **llama.cpp** | 成熟稳定，GGUF格式，CPU优化 | ✅ **首选** |
| MLC LLM | GPU加速，骁龙优化 | 备选 |
| MediaPipe | Google官方，Gemma专用 | Gemma专用 |

## 系统架构

### 混合对话模式

```
用户说话 ──▶ 唤醒词 ──▶ 本地 STT ──▶ 意图分类
                                        │
                          ┌─────────────┴─────────────┐
                          ▼                           ▼
                    智能家居命令                   一般对话
                    (检测到设备关键词)            (其他内容)
                          │                           │
                          ▼                           ▼
                    HA Conversation API          本地 LLM
                    (优先使用)                   (离线可用)
                          │                           │
                          └───────────┬───────────────┘
                                      ▼
                                  TTS 播放
```

### 意图分类逻辑

```kotlin
fun classifyIntent(text: String): Intent {
    // 智能家居关键词检测
    val homeKeywords = listOf("打开", "关闭", "开灯", "关灯", "空调", "温度", "窗帘")
    
    return if (homeKeywords.any { text.contains(it) }) {
        Intent.HomeAssistant  // 发送到 HA
    } else {
        Intent.LocalChat      // 本地 LLM 处理
    }
}
```

## 内存使用预估

```
┌─────────────────────────────────────────────────────────┐
│             12GB RAM 设备内存分配                         │
├─────────────────────────────────────────────────────────┤
│  系统 + 其他应用              ≈ 4GB                      │
│  ────────────────────────────────────                   │
│  NabuKey App                  ≈ 200MB                   │
│  SenseVoice-Small (STT)       ≈ 200MB                   │
│  Qwen3 4B (Q4量化)            ≈ 2.5GB                   │
│  推理时额外内存                ≈ 500MB                   │
│  ────────────────────────────────────                   │
│  总计使用                      ≈ 7.4GB                   │
│  剩余可用                      ≈ 4.6GB ✅                │
└─────────────────────────────────────────────────────────┘
```

## 实现任务

- [ ] 集成 llama.cpp Android 库
- [ ] 下载并集成 Qwen3 4B GGUF 模型
- [ ] 实现模型加载和推理接口
- [ ] 创建意图分类器
- [ ] 实现 HA / 本地 LLM 路由逻辑
- [ ] 添加对话历史管理
- [ ] 添加设置项：启用/禁用本地 LLM、选择模型
- [ ] 优化内存管理和模型卸载

## 文件结构

```
app/src/main/java/com/nabukey/
├── llm/
│   ├── LocalLLM.kt              # LLM 推理封装
│   ├── LLMConfig.kt             # 模型配置
│   └── ChatHistory.kt           # 对话历史管理
├── intent/
│   └── IntentClassifier.kt      # 意图分类器
└── assets/
    └── models/
        └── qwen3-4b-q4.gguf     # 模型文件 (~2.5GB)
```

## 应用场景

| 场景 | 示例 | HA 可用时 | HA 不可用时 |
|-----|------|----------|------------|
| 智能家居 | "打开客厅灯" | HA 处理 | 提示网络问题 |
| 简单问答 | "今天星期几？" | 本地 LLM | 本地 LLM |
| 闲聊 | "讲个笑话" | 本地 LLM | 本地 LLM |
| 计算 | "123加456等于多少" | 本地 LLM | 本地 LLM |

## 注意事项

1. **首次加载**: 模型加载需要 5-10 秒，建议应用启动时预加载
2. **存储空间**: GGUF 模型约 2.5GB，需提醒用户
3. **电量消耗**: 推理时 CPU 占用较高，注意电量
4. **温度控制**: 长时间推理可能导致设备发热

## 相关资源

- [llama.cpp GitHub](https://github.com/ggerganov/llama.cpp)
- [Qwen3 模型](https://huggingface.co/Qwen/Qwen3-4B-GGUF)
- [MLC LLM](https://mlc.ai/mlc-llm/)
